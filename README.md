# ğŸ beeai-litellm-watsonx

## ğŸ“¦ Installation
```bash
pip install -r requirements.txt
chmod +x run_server.sh
```

## âš™ï¸ Setup environment
```bash
cp .env.example .env
```
And edit the `.env` file to set your environment variables.

## ğŸ¤– Choosing a model
You may set a different model in the `config.yaml` file. The default model is `meta-llama/llama-3-3-70b-instruct`.

## ğŸš€ Usage
```bash
./run_server.sh
```

## ğŸ§ª Test with curl
```bash
curl --location 'http://0.0.0.0:4000/chat/completions' \
--header 'Content-Type: application/json' \
--data ' {
      "model": "meta-llama/llama-3-3-70b-instruct",
      "messages": [
        {
          "role": "user",
          "content": "what is your favorite colour?"
        }
      ]
    }
'
```

## ğŸ Setup BeeAI platform environment
_(Assuming you have a BeeAI platform installed and `beeai` cli)_

Make sure the litellm server is running, and then run the following command to setup the BeeAI platform environment:
```bash
beeai env setup
```

```txt
? Select LLM provider: Other               ğŸ”§ provide API URL
? Enter the base URL of your API (OpenAI-compatible): http://localhost:4000
? Enter API key: dummy
? Select a model (type to filter): meta-llama/llama-3-3-70b-instruct

You're all set! (You can re-run this setup anytime with beeai env setup)
```

ğŸ‰ Awesome! Now you can use the BeeAI platform with watsonx.ai!
